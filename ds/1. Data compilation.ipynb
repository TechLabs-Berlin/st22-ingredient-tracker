{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c6a00f",
   "metadata": {},
   "source": [
    "# Ingreduce: Data compilation\n",
    "\n",
    "The recipe data for our web application were collected from the website allrecipes.com. The data collection was done in two steps. First, I collected the urls of recipes from allrecipes.com. Then, I collected each recipe's information from the urls collected.\n",
    "\n",
    "Note that the source where those recipe urls were found, is a file that I created. It contains adresses (that I had found) of files containing the website's recipe urls.\n",
    "\n",
    "## Collecting the recipe urls\n",
    "\n",
    "```Python\n",
    "# First, run \"jupyter notebook --NotebookApp.iopub_data_rate_limit=1e10\" in Terminal.\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def found_urls(soup):\n",
    "    '''Returns a list of all recipe urls found'''\n",
    "    urls_list = []\n",
    "    for loc in soup.find_all('loc'):\n",
    "        url = loc.text\n",
    "        if url.startswith('https://www.allrecipes.com/recipe/') == True:\n",
    "            urls_list.append(url)\n",
    "    return urls_list\n",
    "\n",
    "# Loading a list of files containing the website's recipe urls.\n",
    "\n",
    "filename = 'files_list'\n",
    "infile = open(filename,'rb')\n",
    "files_list = pickle.load(infile)\n",
    "\n",
    "# Appending urls found in files to a list\n",
    "\n",
    "recipe_urls = []\n",
    "\n",
    "for file in files_list:\n",
    "    source = requests.get(file).text\n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "    recipe_urls.extend(found_urls(soup))\n",
    "\n",
    "infile.close()\n",
    "\n",
    "# Removing duplicates\n",
    "recipe_urls = list(set(recipe_urls))\n",
    "\n",
    "# Saving the recipe urls list as pickle\n",
    "\n",
    "filename = 'recipe_urls'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(recipe_urls,outfile)\n",
    "outfile.close()\n",
    "```\n",
    "\n",
    "The outcome is a list of recipe urls saved in a pickle file. Next, we will iterate through each element of that list (that is, each recipe url) to find the corresponding recipe information.\n",
    "\n",
    "## Collecting the recipes' information\n",
    "\n",
    "First, I defined the functions that would scrape the recipes' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ded502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb367e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    ''' Gets the name of recipe from a parsed url object.'''\n",
    "    \n",
    "    name = None\n",
    "    \n",
    "    try:\n",
    "        title_tag = soup.title\n",
    "        name = title_tag.text.split('|')[0].strip()\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_description(soup):\n",
    "    ''' Gets the description of recipe from a parsed url object.'''\n",
    "\n",
    "    description = None\n",
    "    \n",
    "    try:\n",
    "        summary_tag = soup.find('div', class_='recipe-summary elementFont__dek--within')\n",
    "        description = summary_tag.find('p', class_='margin-0-auto').text.strip()\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return description\n",
    "\n",
    "\n",
    "\n",
    "def get_meta(soup):\n",
    "    ''' Gets some meta information of recipe from a parsed url object.'''\n",
    "    \n",
    "    prep = None\n",
    "    cook = None\n",
    "    additional = None\n",
    "    total = None\n",
    "    servings = None\n",
    "    yield_ = None\n",
    "    \n",
    "    try:\n",
    "        container_tag = soup.find('section', class_='recipe-meta-container two-subcol-content clearfix recipeMeta')\n",
    "\n",
    "        for item_tag in container_tag.find_all('div', class_='recipe-meta-item'):\n",
    "            category_name = item_tag.find('div', class_='recipe-meta-item-header elementFont__subtitle--bold elementFont__transformCapitalize').text\n",
    "            meta_data = item_tag.find('div', class_='recipe-meta-item-body elementFont__subtitle').text.strip()\n",
    "\n",
    "            if category_name == 'prep:':\n",
    "                prep = meta_data\n",
    "            if category_name == 'cook:':\n",
    "                cook = meta_data\n",
    "            if category_name == 'additional:':\n",
    "                additional = meta_data\n",
    "            if category_name == 'total:':\n",
    "                total = meta_data\n",
    "            if category_name == 'Servings:':\n",
    "                servings = meta_data\n",
    "            if category_name == 'Yield:':\n",
    "                yield_ = meta_data\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return prep, cook, additional, total, servings, yield_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ingredients(soup):\n",
    "    ''' Gets the ingredient list of recipe from a parsed url object.'''\n",
    "    \n",
    "    ingredients = None\n",
    "    \n",
    "    try:\n",
    "        container_tag = soup.find('section', class_='component recipe-ingredients recipeIngredients container interactive')\n",
    "        \n",
    "        ingredient_list = []\n",
    "    \n",
    "        for item_tag in container_tag.find_all('span', class_='ingredients-item-name elementFont__body'):\n",
    "            formatted_ingredient = item_tag.text.strip()\n",
    "            ingredient_list.append(formatted_ingredient)\n",
    "\n",
    "        if len(ingredient_list) > 1:\n",
    "            ingredients = '; '.join(ingredient_list)\n",
    "        if len(ingredient_list) == 1:\n",
    "            ingredients = ingredient_list[0]\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return ingredients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_instructions(soup):\n",
    "    ''' Gets the instructions for recipe from a parsed url object.'''\n",
    "    \n",
    "    instructions = None\n",
    "    \n",
    "    try:\n",
    "        section_tag = soup.find('ul', class_='instructions-section')\n",
    "\n",
    "        instruction_list = []\n",
    "\n",
    "        for p_tag in section_tag.find_all('p'):\n",
    "            instruction_list.append(p_tag.text)\n",
    "            \n",
    "        if len(instruction_list) > 1:\n",
    "            instructions = ' '.join(instruction_list)\n",
    "        if len(instruction_list) == 1:\n",
    "            instructions = instruction_list[0]\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return instructions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_nutrition(soup):\n",
    "    ''' Gets the nutrition facts per serving of recipe from a parsed url object.'''\n",
    "    \n",
    "    nutrition = None\n",
    "    \n",
    "    try:\n",
    "        section_tag = soup.find('div', class_='recipeNutritionSectionBlock')\n",
    "        body_tag = section_tag.find('div', class_='section-body')\n",
    "        nutrition = body_tag.text.split('Full')[0].strip()\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return nutrition\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_images_url(soup):\n",
    "    ''' Gets the url of recipe's images from a parsed url object.'''\n",
    "    \n",
    "    images_url = None\n",
    "    \n",
    "    # Saving the urls found in a list first, and joining them later if needed\n",
    "    list_images_url = []\n",
    "    \n",
    "    \n",
    "    # Searching images in location 1\n",
    "    try: \n",
    "        aside_tag = soup.find('aside', class_='recipe-tout-image recipe-info-items-3')\n",
    "        \n",
    "        # Look for image of aspect_1x1 \n",
    "        div_tag = aside_tag.find('div', class_='component lazy-image lazy-image-udf aspect_1x1 cache-only align-default')\n",
    "        \n",
    "        # If not found, look for image of aspect_3x2\n",
    "        if div_tag == None:\n",
    "            div_tag = aside_tag.find('div', class_='component lazy-image lazy-image-udf aspect_3x2 cache-only align-default')\n",
    "        \n",
    "        list_images_url.append(div_tag['data-src'])\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Searching images in location 2\n",
    "    try:\n",
    "        image_filmstrip = soup.find('div', class_='component image-filmstrip')\n",
    "\n",
    "        for image_slide in image_filmstrip.find_all('div', class_='image-slide')[1:]:\n",
    "            a_tag = image_slide.find('a', class_='ugc-photos-link')\n",
    "            \n",
    "            # Look for image of aspect_3x2\n",
    "            div_tag = a_tag.find('div', class_='component lazy-image lazy-image-udf aspect_3x2')\n",
    "            \n",
    "            # If not found, look for image of aspect_3x4\n",
    "            if div_tag == None:\n",
    "                div_tag = a_tag.find('div', class_='component lazy-image lazy-image-udf aspect_3x4')\n",
    "            \n",
    "            # If not found, look for image of aspect_1x1\n",
    "            if div_tag == None:\n",
    "                div_tag = a_tag.find('div', class_='component lazy-image lazy-image-udf aspect_1x1')\n",
    "\n",
    "            list_images_url.append(div_tag['data-src'])\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # Joining the urls in the list if applicable\n",
    "    \n",
    "    if len(list_images_url) > 1:\n",
    "        images_url = '; '.join(list_images_url)\n",
    "    if len(list_images_url) == 1:\n",
    "        images_url = list_images_url[0]\n",
    "\n",
    "    return images_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipeinfodict(recipe_url):\n",
    "    ''' Returns a dictionary of the recipe information found on a given allrecipes recipe webpage.'''\n",
    "    \n",
    "    recipe_info = {'name': None,\n",
    "                   'description': None,\n",
    "                   'prep': None,\n",
    "                   'cook': None,\n",
    "                   'additional': None,\n",
    "                   'total': None,\n",
    "                   'servings': None,\n",
    "                   'yield': None,\n",
    "                   'ingredients': None,\n",
    "                   'instructions': None,\n",
    "                   'nutrition': None,\n",
    "                   'images_url': None\n",
    "                  }\n",
    "    \n",
    "    # Proceeding with scraping only if the request works\n",
    "    \n",
    "    r = requests.head(recipe_url)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        source = requests.get(recipe_url).text\n",
    "        soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        \n",
    "        # Combining all of the information into a dict\n",
    "\n",
    "        recipe_info = {'name': get_name(soup), \n",
    "                       'description': get_description(soup), \n",
    "                       'prep': get_meta(soup)[0], \n",
    "                       'cook': get_meta(soup)[1], \n",
    "                       'additional': get_meta(soup)[2], \n",
    "                       'total': get_meta(soup)[3], \n",
    "                       'servings': get_meta(soup)[4], \n",
    "                       'yield': get_meta(soup)[5], \n",
    "                       'ingredients': get_ingredients(soup), \n",
    "                       'instructions': get_instructions(soup), \n",
    "                       'nutrition': get_nutrition(soup), \n",
    "                       'images_url': get_images_url(soup)\n",
    "                      }\n",
    "    \n",
    "    return recipe_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694290b",
   "metadata": {},
   "source": [
    "Now we can use the recipe urls list and start collecting the recipes' information in a DataFrame.\n",
    "\n",
    "```Python\n",
    "\n",
    "# Loading the recipe urls list\n",
    "\n",
    "filename = 'recipe_urls'\n",
    "infile = open(filename,'rb')\n",
    "recipe_urls = pickle.load(infile)\n",
    "\n",
    "\n",
    "# Creating a DataFrame that will store the scraped information\n",
    "\n",
    "headers = ['recipe_link', 'name', 'description', 'prep', 'cook', 'additional', 'total', 'servings', 'yield', 'ingredients', 'instructions', 'nutrition', 'images_url']\n",
    "df = pd.DataFrame(columns=headers)\n",
    "\n",
    "df['recipe_link'] = recipe_urls\n",
    "```\n",
    "\n",
    "While I could have run the following code block and got the entire dataset in one go (with `range(df.shape[0])`), I split the scraping into batches of 1000. There was two reasons for this:\n",
    "- I wanted to frequently save the scraped data.\n",
    "- Additionally, it allowed me to manually check whether my scraping code would work with all the recipe urls, as the website has been up for two decades and may contain inconsistent formatting, moved urls, etc. \n",
    "\n",
    "```Python\n",
    "\n",
    "# Inputting recipe information into the DataFrame from the urls of the list\n",
    "\n",
    "for index in range(1000):\n",
    "    df.loc[index, 'name'] = recipeinfodict(df.loc[index, 'recipe_link'])['name']\n",
    "    df.loc[index, 'description'] = recipeinfodict(df.loc[index, 'recipe_link'])['description']\n",
    "    df.loc[index, 'prep'] = recipeinfodict(df.loc[index, 'recipe_link'])['prep']\n",
    "    df.loc[index, 'cook'] = recipeinfodict(df.loc[index, 'recipe_link'])['cook']\n",
    "    df.loc[index, 'additional'] = recipeinfodict(df.loc[index, 'recipe_link'])['additional']\n",
    "    df.loc[index, 'total'] = recipeinfodict(df.loc[index, 'recipe_link'])['total']\n",
    "    df.loc[index, 'servings'] = recipeinfodict(df.loc[index, 'recipe_link'])['servings']\n",
    "    df.loc[index, 'yield'] = recipeinfodict(df.loc[index, 'recipe_link'])['yield']\n",
    "    df.loc[index, 'ingredients'] = recipeinfodict(df.loc[index, 'recipe_link'])['ingredients']\n",
    "    df.loc[index, 'instructions'] = recipeinfodict(df.loc[index, 'recipe_link'])['instructions']\n",
    "    df.loc[index, 'nutrition'] = recipeinfodict(df.loc[index, 'recipe_link'])['nutrition']\n",
    "    df.loc[index, 'images_url'] = recipeinfodict(df.loc[index, 'recipe_link'])['images_url']\n",
    "    print(df.loc[index, 'recipe_link'])\n",
    "    time.sleep(1)\n",
    "\n",
    "# Saving the DataFrame of the data scraped\n",
    "\n",
    "filename = 'df1000'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(df,outfile)\n",
    "outfile.close()\n",
    "\n",
    "# Opening the DataFrame of the data scraped\n",
    "\n",
    "filename = 'df1000'\n",
    "infile = open(filename,'rb')\n",
    "df = pickle.load(infile)\n",
    "\n",
    "# Inputting recipe information into the DataFrame previously saved\n",
    "\n",
    "for index in range(1000, 2000):\n",
    "    df.loc[index, 'name'] = recipeinfodict(df.loc[index, 'recipe_link'])['name']\n",
    "    df.loc[index, 'description'] = recipeinfodict(df.loc[index, 'recipe_link'])['description']\n",
    "    df.loc[index, 'prep'] = recipeinfodict(df.loc[index, 'recipe_link'])['prep']\n",
    "    df.loc[index, 'cook'] = recipeinfodict(df.loc[index, 'recipe_link'])['cook']\n",
    "    df.loc[index, 'additional'] = recipeinfodict(df.loc[index, 'recipe_link'])['additional']\n",
    "    df.loc[index, 'total'] = recipeinfodict(df.loc[index, 'recipe_link'])['total']\n",
    "    df.loc[index, 'servings'] = recipeinfodict(df.loc[index, 'recipe_link'])['servings']\n",
    "    df.loc[index, 'yield'] = recipeinfodict(df.loc[index, 'recipe_link'])['yield']\n",
    "    df.loc[index, 'ingredients'] = recipeinfodict(df.loc[index, 'recipe_link'])['ingredients']\n",
    "    df.loc[index, 'instructions'] = recipeinfodict(df.loc[index, 'recipe_link'])['instructions']\n",
    "    df.loc[index, 'nutrition'] = recipeinfodict(df.loc[index, 'recipe_link'])['nutrition']\n",
    "    df.loc[index, 'images_url'] = recipeinfodict(df.loc[index, 'recipe_link'])['images_url']\n",
    "    print(df.loc[index, 'recipe_link'])\n",
    "    time.sleep(1)\n",
    "\n",
    "# Saving the DataFrame of the data scraped\n",
    "\n",
    "filename = 'df2000'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(df,outfile)\n",
    "outfile.close()\n",
    "```\n",
    "\n",
    "And so on, until the 54416 urls found have been scraped. Or at least this was the plan. Time constraint and a slow internet connection only allowed me to scrape **4840** recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8f468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
